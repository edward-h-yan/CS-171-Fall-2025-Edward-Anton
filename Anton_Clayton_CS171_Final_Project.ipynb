{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1PKOA-2A38YmN3_XUW8l8Wl17pIyJMl1k",
      "authorship_tag": "ABX9TyN13NudGz8A5ocOcqADUPGD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS171 Final Project\n",
        "### By Anton Clayton\n",
        "#### Dataset Link: https://www.kaggle.com/datasets/alessiocorrado99/animals10"
      ],
      "metadata": {
        "id": "BeI5UVescYqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ],
      "metadata": {
        "id": "JCvCAvGAktbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPTION 1: If you put the dataset ZIP file into google drive\n",
        "Run this if the file animals-dataset.zip is already in your Drive folder.\n"
      ],
      "metadata": {
        "id": "YnL-WeAWhi5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")"
      ],
      "metadata": {
        "id": "WUgmtC4gEbkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebed9fd2"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Copy and Unzip\n",
        "# Update this path if zip is named differently or in a different folder\n",
        "drive_zip_path = '/content/drive/MyDrive/CS171_animals_dataset/animals-dataset.zip'\n",
        "local_extract_path = '/content/animal_data'\n",
        "\n",
        "print(\"Copying and unzipping from Drive...\")\n",
        "shutil.copy(drive_zip_path, '/content/temp.zip')\n",
        "shutil.unpack_archive('/content/temp.zip', local_extract_path)\n",
        "\n",
        "# Setup variables for loop\n",
        "base_path = f'{local_extract_path}/raw-img'\n",
        "image_filepaths = []\n",
        "image_labels = []\n",
        "\n",
        "print(f\"'base_path' is set to: {base_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPTION 2: If you upload the dataset ZIP file locally/manually\n",
        "Run this if you just dragged and dropped animals-dataset.zip into the Colab \"Files\" sidebar."
      ],
      "metadata": {
        "id": "FMjrMv2hhok3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Unzip the uploaded file\n",
        "# Make sure the uploaded file is named exactly this, or change the string below\n",
        "uploaded_zip_name = 'animals-dataset.zip'\n",
        "local_zip_path = f'/content/{uploaded_zip_name}'\n",
        "local_extract_path = '/content/animal_data'\n",
        "\n",
        "if os.path.exists(local_zip_path):\n",
        "    print(f\"Unzipping {uploaded_zip_name}...\")\n",
        "    shutil.unpack_archive(local_zip_path, local_extract_path)\n",
        "\n",
        "    # Setup variables for loop\n",
        "    base_path = f'{local_extract_path}/raw-img'\n",
        "    image_filepaths = []\n",
        "    image_labels = []\n",
        "\n",
        "    print(f\"Done! 'base_path' is set to: {base_path}\")\n",
        "else:\n",
        "    print(f\"Error: Could not find '{uploaded_zip_name}'. Did you upload it to the Files sidebar?\")"
      ],
      "metadata": {
        "id": "Tcqglmpoh2FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7991a44"
      },
      "source": [
        "from collections import defaultdict\n",
        "# Initialize a defaultdict to count images per class, limited to 1000\n",
        "class_counts = defaultdict(int)\n",
        "limit_per_class = 1000\n",
        "\n",
        "print(f\"Starting image collection with a limit of {limit_per_class} images per class...\")\n",
        "\n",
        "for class_name in os.listdir(base_path):\n",
        "    class_path = os.path.join(base_path, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        # Only process if the class count is below the limit or not yet started\n",
        "        if class_counts[class_name] < limit_per_class:\n",
        "            for img_name in os.listdir(class_path):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    if class_counts[class_name] < limit_per_class:\n",
        "                        img_path = os.path.join(class_path, img_name)\n",
        "                        image_filepaths.append(img_path)\n",
        "                        image_labels.append(class_name)\n",
        "                        class_counts[class_name] += 1\n",
        "                    else:\n",
        "                        # Already collected 1000 images for this class, move to next file\n",
        "                        break # Exit inner loop for this class\n",
        "\n",
        "print(f\"Finished collecting image paths. Total image paths collected: {len(image_filepaths)}\")\n",
        "print(\"Number of images collected per class (limited to 1000):\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"- {class_name}: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea9d88dc"
      },
      "source": [
        "import collections\n",
        "from PIL import Image\n",
        "\n",
        "# Print the total number of images\n",
        "print(f\"Total number of images loaded: {len(image_filepaths)}\")\n",
        "\n",
        "# Count and display the number of images for each class to confirm 1000\n",
        "class_counts = collections.Counter(image_labels)\n",
        "print(\"\\nNumber of images per class:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"- {class_name}: {count}\")\n",
        "\n",
        "# Select and display the dimensions of a few sample images\n",
        "print(\"\\nDimensions of a few sample images:\")\n",
        "# Display dimensions for the first 5 images just to see\n",
        "for i in range(min(5, len(image_filepaths))):\n",
        "    img_path = image_filepaths[i]\n",
        "    try:\n",
        "        with Image.open(img_path) as img_obj:\n",
        "            print(f\"- Image {i+1} (Class: {image_labels[i]}): {img_obj.size[0]}x{img_obj.size[1]} pixels\")\n",
        "    except Exception as e:\n",
        "        print(f\"- Could not open image {img_path}: {e}\")\n",
        "\n",
        "# Display dimensions for a few images from the middle of the dataset just to see\n",
        "if len(image_filepaths) > 10:\n",
        "    for i in range(len(image_filepaths) // 2, min(len(image_filepaths) // 2 + 3, len(image_filepaths))):\n",
        "        img_path = image_filepaths[i]\n",
        "        try:\n",
        "            with Image.open(img_path) as img_obj:\n",
        "                print(f\"- Image {i+1} (Class: {image_labels[i]}): {img_obj.size[0]}x{img_obj.size[1]} pixels\")\n",
        "        except Exception as e:\n",
        "            print(f\"- Could not open image {img_path}: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "745825a8"
      },
      "source": [
        "# Map labels from Italian to English\n",
        "translation_map = {\n",
        "    \"cane\": \"dog\",\n",
        "    \"cavallo\": \"horse\",\n",
        "    \"elefante\": \"elephant\",\n",
        "    \"farfalla\": \"butterfly\",\n",
        "    \"gallina\": \"chicken\",\n",
        "    \"gatto\": \"cat\",\n",
        "    \"mucca\": \"cow\",\n",
        "    \"pecora\": \"sheep\",\n",
        "    \"scoiattolo\": \"squirrel\",\n",
        "    \"ragno\": \"spider\"\n",
        "}\n",
        "\n",
        "# Translate labels in-place\n",
        "for i in range(len(image_labels)):\n",
        "    image_labels[i] = translation_map.get(image_labels[i], image_labels[i])\n",
        "\n",
        "print(\"Labels translated successfully.\")\n",
        "\n",
        "# Verify a few sample translated labels\n",
        "print(\"\\nSample translated labels (first 5):\")\n",
        "for i in range(min(5, len(image_labels))):\n",
        "    print(f\"- {image_labels[i]}\")\n",
        "\n",
        "# Pprint counts of each translated label (sanity check)\n",
        "from collections import Counter\n",
        "translated_class_counts = Counter(image_labels)\n",
        "print(\"\\nNumber of images per translated class:\")\n",
        "for class_name, count in translated_class_counts.items():\n",
        "    print(f\"- {class_name}: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632ce2f1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    image_filepaths,\n",
        "    image_labels,\n",
        "    test_size=0.2,\n",
        "    stratify=image_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Total processed images: {len(image_filepaths)}\")\n",
        "print(f\"Training set size: {len(X_train)} images\")\n",
        "print(f\"Testing set size: {len(X_test)} images\")\n",
        "\n",
        "# Verify the class distribution in training and testing sets\n",
        "from collections import Counter\n",
        "\n",
        "print(\"\\nTraining set class distribution:\")\n",
        "for label, count in Counter(y_train).items():\n",
        "    print(f\"- {label}: {count}\")\n",
        "\n",
        "print(\"\\nTesting set class distribution:\")\n",
        "for label, count in Counter(y_test).items():\n",
        "    print(f\"- {label}: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02663734"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# Define Transforms (both produce 3x224x224 tensor)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),        # Resize slightly larger for next step\n",
        "    transforms.RandomResizedCrop(224),    # Crop a random part (Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),    # Flip horizontally (Augmentation)\n",
        "    transforms.ToTensor(),                # Converts to [0,1] (replaces / 255.0)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet stats\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),        # specific target size (for consistency)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a9cf802"
      },
      "source": [
        "### Define Label Mapping\n",
        "\n",
        "We need to create a mapping from string labels to integer labels, as deep learning models typically work with numerical labels. This will also ensure consistent indexing for our 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab1c7122"
      },
      "source": [
        "# Lazy-Loading Dataset (to avoid Colab RAM/memory issues)\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, label_to_int, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.label_to_int = label_to_int\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image ONLY when training loop asks for it\n",
        "        img_path = self.image_paths[idx]\n",
        "        # .convert('RGB') fixes the grayscale/RGBA issues automatically\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        label_str = self.labels[idx]\n",
        "        label_int = self.label_to_int[label_str]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Datasets and Loaders\n",
        "unique_labels = sorted(list(set(image_labels)))\n",
        "label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "train_dataset = CustomImageDataset(X_train, y_train, label_to_int, transform=train_transforms)\n",
        "test_dataset = CustomImageDataset(X_test, y_test, label_to_int, transform=test_transforms)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(f\"Train Loader ready with {len(train_dataset)} images.\")\n",
        "print(f\"Test Loader ready with {len(test_dataset)} images.\")"
      ],
      "metadata": {
        "id": "xgGpKegkJlYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Construction"
      ],
      "metadata": {
        "id": "tZbmMhnNk-wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AnimalClassificationCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AnimalClassificationCNN, self).__init__()\n",
        "\n",
        "        # --- CONVOLUTIONAL LAYERS ---\n",
        "        # Block 1: Input (3, 224, 224) -> Output (32, 112, 112)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Reduces Image Size by half\n",
        "\n",
        "        # Block 2: Input (32, 112, 112) -> Output (64, 56, 56)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Block 3: Input (64, 56, 56) -> Output (128, 28, 28)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        # --- FULLY CONNECTED LAYERS ---\n",
        "        # The input to the linear layer is: Channels * Height * Width\n",
        "        # Based on the blocks above: 128 * 28 * 28 = 100,352 features\n",
        "        self.flatten_size = 128 * 28 * 28\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 512)\n",
        "        self.dropout = nn.Dropout(0.5) # Helps prevent overfitting\n",
        "        self.fc2 = nn.Linear(512, num_classes) # Output layer (10 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # Block 2\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # Block 3\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        # Flatten: Turn the 3D image cubes into a 1D vector\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "\n",
        "        # Classification Head\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x) # No softmax here! CrossEntropyLoss handles it.\n",
        "        return x"
      ],
      "metadata": {
        "id": "nDvbVvJwaFnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 1. Detect if we have a GPU available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "# 2. Initialize the model and move it to the GPU\n",
        "model = AnimalClassificationCNN(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "# 3. Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Model initialized successfully.\")"
      ],
      "metadata": {
        "id": "L2lgNuveaqTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set number of epochs\n",
        "num_epochs = 30\n",
        "\n",
        "# Lists to store metrics\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # --- TRAINING ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Wrap the train_loader with tqdm to show progress bar\n",
        "    # desc sets the text description for the bar\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", unit=\"batch\")\n",
        "\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track stats\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update the progress bar with current loss\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_running_loss / len(test_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Complete! \"\n",
        "          f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "id": "Qe-yiq62a5M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Analysis and Visualization"
      ],
      "metadata": {
        "id": "N-YIszFflBnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7yCZqbgHbIJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more coming"
      ],
      "metadata": {
        "id": "FiekDDvFqBv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}